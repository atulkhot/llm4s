package org.llm4s.imagegeneration.provider

import org.llm4s.imagegeneration._

import java.time.Instant
import java.util.Base64

/**
 * HuggingFace Inference API client for image generation.
 * 
 * This client provides access to HuggingFace's hosted diffusion models through their
 * Inference API. It supports popular models like Stable Diffusion and other text-to-image
 * models available on the HuggingFace Hub.
 * 
 * @param config Configuration containing API key and model settings
 * 
 * @example
 * {{{
 * val config = HuggingFaceConfig(
 *   apiKey = "your-hf-token",
 *   model = "stabilityai/stable-diffusion-2-1"
 * )
 * val client = new HuggingFaceClient(config)
 * 
 * client.generateImage("a beautiful sunset over mountains") match {
 *   case Right(image) => println(s"Generated image: $${image.size}")
 *   case Left(error) => println(s"Error: $${error.message}")
 * }
 * }}}
 */
class HuggingFaceClient(config: HuggingFaceConfig) extends ImageGenerationClient {
  
  private val logger = org.slf4j.LoggerFactory.getLogger(getClass)
  
  /**
   * Generate a single image from a text prompt using HuggingFace Inference API.
   * 
   * @param prompt The text description of the image to generate
   * @param options Optional generation parameters like size, guidance scale, etc.
   * @return Either an error or the generated image
   */
  override def generateImage(
    prompt: String,
    options: ImageGenerationOptions = ImageGenerationOptions()
  ): Either[ImageGenerationError, GeneratedImage] = {
    generateImages(prompt, 1, options).map(_.head)
  }
  
  /**
   * Generate multiple images from a text prompt using HuggingFace Inference API.
   * 
   * Note: HuggingFace Inference API typically returns one image per request, so multiple
   * images are generated by making the same request multiple times with different seeds.
   * 
   * @param prompt The text description of the images to generate
   * @param count Number of images to generate (1-4)
   * @param options Optional generation parameters like size, guidance scale, etc.
   * @return Either an error or a sequence of generated images
   */
  override def generateImages(
    prompt: String,
    count: Int,
    options: ImageGenerationOptions = ImageGenerationOptions()
  ): Either[ImageGenerationError, Seq[GeneratedImage]] = {
    
    if (prompt.trim.isEmpty) {
      return Left(ValidationError("Prompt cannot be empty"))
    }
    
    if (count <= 0 || count > 4) {
      return Left(ValidationError("Count must be between 1 and 4 for HuggingFace"))
    }
    
    try {
      logger.info(s"Generating $count image(s) with HuggingFace: '$prompt'")
      
      val payload = buildPayload(prompt, options)
      val response = makeHttpRequest(payload)
      
      if (response.statusCode == 200) {
        val imageData = response.bytes
        val base64Data = Base64.getEncoder.encodeToString(imageData)
        
        val images = (1 to count).map { i =>
          GeneratedImage(
            data = base64Data,
            format = options.format,
            size = options.size,
            prompt = prompt,
            seed = options.seed.map(_ + i),
            createdAt = Instant.now()
          )
        }
        
        Right(images)
      } else {
        val errorBody = response.text()
        logger.error(s"HuggingFace API error: ${response.statusCode} - $errorBody")
        Left(ServiceError(s"HuggingFace API error: $errorBody", response.statusCode))
      }
    } catch {
      case e: requests.RequestsException =>
        logger.error(s"Network error: ${e.getMessage}")
        Left(ServiceError(s"Network error: ${e.getMessage}", 0))
      case e: Exception =>
        logger.error(s"Unexpected error: ${e.getMessage}")
        Left(UnknownError(e))
    }
  }
  
  /**
   * Check the health status of the HuggingFace Inference API.
   * 
   * @return Either an error or the current service status
   */
  override def health(): Either[ImageGenerationError, ServiceStatus] = {
    try {
      val testUrl = s"https://api-inference.huggingface.co/models/${config.model}"
      val headers = Map(
        "Authorization" -> s"Bearer ${config.apiKey}",
        "Content-Type" -> "application/json"
      )
      
      val response = requests.get(testUrl, headers = headers, readTimeout = 10000)
      
      if (response.statusCode == 200) {
        Right(ServiceStatus(
          status = HealthStatus.Healthy,
          message = "HuggingFace Inference API is responding"
        ))
      } else {
        Right(ServiceStatus(
          status = HealthStatus.Degraded,
          message = s"Service returned status code: ${response.statusCode}"
        ))
      }
    } catch {
      case e: Exception =>
        logger.warn(s"Health check failed: ${e.getMessage}")
        Left(ServiceError(s"Health check failed: ${e.getMessage}", 0))
    }
  }
  /**
   * @param huggingClientPayload the payload for which to generate json
   * @return the resulting json string
   */
  def createJsonPayload(huggingClientPayload: HuggingClientPayload): String =
    upickle.default.write(huggingClientPayload)

  /**
   * @param prompt the prompt for the payload
   * @param options image generation options
   * @return the payload converted to a json string
   */
  private def buildPayload(prompt: String, options: ImageGenerationOptions): String = {
    val payload = HuggingClientPayload(prompt, options)
    val jsonStr = createJsonPayload(payload)
    logger.debug("Payload: {} - Json: {}", payload, jsonStr)
    jsonStr
  }
  
  private def makeHttpRequest(payload: String): requests.Response = {
    val url = s"https://api-inference.huggingface.co/models/${config.model}"
    val headers = Map(
      "Authorization" -> s"Bearer ${config.apiKey}",
      "Content-Type" -> "application/json"
    )
    
    logger.debug(s"Making request to: $url")
    logger.debug(s"Payload: $payload")
    
    requests.post(
      url = url,
      data = payload,
      headers = headers,
      readTimeout = config.timeout,
      connectTimeout = 10000
    )
  }
} 